{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2cc0db9-39b7-4b28-8586-c9b44478004f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "MAKE SURE YOU HAVE COMPILED EVERYTHING BEFOREHAND (run all the lines it asks in the read me)\n",
    "FILE PATHS MUST NOT HAVE SPACES!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf707cc-46b9-443d-baf5-bb15148c3c72",
   "metadata": {},
   "source": [
    "To get started, run the 8 cells below this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cb36e99-556e-4da2-9b1d-9d8b9eea4e64",
   "metadata": {
    "id": "6cb36e99-556e-4da2-9b1d-9d8b9eea4e64"
   },
   "outputs": [],
   "source": [
    "def hamming_distance(str1, str2) -> int:\n",
    "    if len(str1) != len(str2):\n",
    "        raise ValueError(\"Input strings must have the same length\")\n",
    "\n",
    "    distance = 0\n",
    "    str1 = str1.upper()\n",
    "    str2 = str2.upper()\n",
    "    for i in range(len(str1)):\n",
    "        if str1[i] == 'N': # character N does not match with anything\n",
    "            distance += 1\n",
    "        elif str1[i] != str2[i]:\n",
    "            distance += 1\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51060584-1685-410c-99ea-0641d02d87ee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "error",
     "timestamp": 1712959613483,
     "user": {
      "displayName": "Mete Minbay '24",
      "userId": "02702889252066806519"
     },
     "user_tz": 240
    },
    "id": "51060584-1685-410c-99ea-0641d02d87ee",
    "outputId": "a9c10132-4466-4909-a7b5-e496b2373515"
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "def synthesize_sequence(\n",
    "    s_length: int,\n",
    "    n_repeats: int,\n",
    "    repeat_length: Union[int, Tuple[int, int]],\n",
    "    repeat_coverage: float,\n",
    "    repeat_noise: int,\n",
    "    log: str = None\n",
    ") -> List[str]:\n",
    "    '''\n",
    "    Generate a randomized nucleotide sequence with the given parameters.\n",
    "\n",
    "    Arguments:\n",
    "        s_length (int): Length of the sequence to be generated.\n",
    "        n_repeats (int): Number of unique repeats. These repeats will be randomly\n",
    "            scattered around the generated sequence.\n",
    "        repeat_length (int or tuple[int, int]): Value, or min and max values for the length of the repeats.\n",
    "        If a tuple if provided, the repeats will be randomized within this range.\n",
    "        repeat_coverage ([0, 1]): The proportion of the finals sequence that should be populated\n",
    "            with repeats. Note that this is a lower bound: The randomized regions may have repeats\n",
    "            by chance.\n",
    "        repeat_noise (int): Maximum Hamming distance to randomize repeats with. Once a repeat is\n",
    "            decided, the sequence will be populated with its variants that are at most this distance\n",
    "            away from that repeat.\n",
    "        log (str): If provided, will output a log of the process to the provided path.\n",
    "    '''\n",
    "    print(\"Creating string\",s_length,n_repeats,repeat_length,repeat_coverage,repeat_noise,log)\n",
    "    verbose = False\n",
    "    if log is not None:\n",
    "        verbose = True\n",
    "        f = open(log, 'w')\n",
    "        f.write('Generating baits with following arguments:\\n')\n",
    "        f.write('L = {}\\n'.format(s_length))\n",
    "        f.write('RN (repeat number) = {}\\n'.format(n_repeats))\n",
    "        f.write('RL (repeat length) = {}\\n'.format(repeat_length))\n",
    "        f.write('RC (repeat coverage) = {}\\n'.format(repeat_coverage))\n",
    "        f.write('RE (repeat error) = {}\\n'.format(repeat_noise))\n",
    "\n",
    "    BASES = ['A', 'G', 'T', 'C']\n",
    "    vec = [None] * s_length\n",
    "    total_populated = 0\n",
    "    repeats_pools = {}\n",
    "    if verbose:\n",
    "        repeats_locations = {}\n",
    "    for i in range(n_repeats):\n",
    "        l = repeat_length\n",
    "        if type(repeat_length) is tuple:\n",
    "            l = random.randint(repeat_length[0], repeat_length[1])\n",
    "        repeat = ''.join(random.choices(BASES, k = l))\n",
    "        while repeat in repeats_pools.keys():\n",
    "            repeat = ''.join(random.choices(BASES, k = l))\n",
    "        repeats_pools[repeat] = set(range(s_length - l + 1))\n",
    "        if verbose:\n",
    "            repeats_locations[repeat] = []\n",
    "    repeat_coverage = repeat_coverage * s_length\n",
    "    repeats = list(repeats_pools.keys())\n",
    "    repeats_ctr = 0\n",
    "    while total_populated < repeat_coverage:\n",
    "        repeat = repeats[repeats_ctr]\n",
    "        i = -1\n",
    "        if repeat_coverage == s_length:\n",
    "            i = total_populated # if entire sequence is repeats, fill the next index\n",
    "        elif len(repeats_pools[repeat]) == 0:\n",
    "            if verbose:\n",
    "                f.write('WARNING: {} has no more indices it can be planted in.\\n'.format(repeat))\n",
    "            repeats.remove(repeat)\n",
    "            n_repeats -= 1\n",
    "            repeats_ctr = repeats_ctr % n_repeats\n",
    "            continue\n",
    "        else:\n",
    "            i = random.choice(list(repeats_pools[repeat]))\n",
    "        if i + len(repeat) > s_length:\n",
    "            if repeat_coverage == s_length:\n",
    "                vec.extend([None] * (i + len(repeat) - s_length)) # extend vector to accomodate this plant\n",
    "                if verbose:\n",
    "                    f.write('Extended sequence to {} base pairs to add final repeat.'.format(len(vec)))\n",
    "            else:\n",
    "                continue\n",
    "        for j in range(len(repeat)):\n",
    "            if vec[i + j] is not None:\n",
    "                raise Exception('this shouldnt happen')\n",
    "        if verbose:\n",
    "            repeats_locations[repeat].append(i)\n",
    "        n_modifs = random.randint(0, repeat_noise)\n",
    "        modif_locs = random.choices(range(len(repeat)), k = n_modifs)\n",
    "        for j in range(len(repeat)):\n",
    "            if j in modif_locs:\n",
    "                vec[i + j] = random.choice([x for x in BASES if x != repeat[j]])\n",
    "            else:\n",
    "                vec[i + j] = repeat[j]\n",
    "        for other in repeats:\n",
    "            for j in range(i - len(other) + 1, i + len(repeat)):\n",
    "                if j in repeats_pools[other]:\n",
    "                    repeats_pools[other].discard(j)\n",
    "        repeats_ctr = (repeats_ctr + 1) % n_repeats\n",
    "        total_populated = len(vec) - vec.count(None)\n",
    "    if verbose:\n",
    "        f.write('Total covered base pairs: {}\\n'.format(total_populated))\n",
    "        for k, v in repeats_locations.items():\n",
    "            if len(v) == 0 or len(v) == 1:\n",
    "                print('WARNING: Some repeats were planted < 2 times')\n",
    "                f.write('WARNING: Some repeats were planted < 2 times\\n')\n",
    "                break\n",
    "        f.write('Repeats and the locations they were planted in:\\n')\n",
    "        for k, v in repeats_locations.items():\n",
    "            f.write(str(k) +  ': ' + str(v) + '\\n')\n",
    "    for i in range(s_length):\n",
    "        if vec[i] is None:\n",
    "            vec[i] = random.choice(BASES)\n",
    "    return ''.join(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3a78b9d-734d-431b-92b1-bcf971587566",
   "metadata": {
    "id": "b3a78b9d-734d-431b-92b1-bcf971587566"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "qBMWNS2WPjjU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1712959789916,
     "user": {
      "displayName": "Mete Minbay '24",
      "userId": "02702889252066806519"
     },
     "user_tz": 240
    },
    "id": "qBMWNS2WPjjU",
    "outputId": "4d9b3a2d-8b99-4f52-97c2-1466f6839e3c"
   },
   "outputs": [],
   "source": [
    "from typing import List, Union, Tuple\n",
    "import numpy as np\n",
    "def initialize_ignore_vector(\n",
    "  seqlens: List[int],\n",
    "  l: int,\n",
    ") -> List[bool]:\n",
    "  '''\n",
    "  Given a list of sequence lengths, create an ignore vector where the indices that would\n",
    "  cover concatenation spots are marked. Concatenation spots cannot be aligned to with baits,\n",
    "  so they never need to be checked for alignments and can be ignored.\n",
    "  '''\n",
    "  ignore = [False] * sum(seqlens)\n",
    "  seqlen_sum = 0\n",
    "  for seqlen in seqlens:\n",
    "    seqlen_sum += seqlen\n",
    "    for i in range(seqlen_sum - l + 1, seqlen_sum):\n",
    "      ignore[i] = True\n",
    "  return ignore\n",
    "\n",
    "def update_ignore_vector(\n",
    "  cov: List[bool],\n",
    "  seqlens: List[int],\n",
    "  ignore: List[bool],\n",
    "  region: Tuple[int, int],\n",
    "  l: int\n",
    ") -> None:\n",
    "  '''\n",
    "  Given a coverage vector, an ignore vector, and a covered region, find the indices that\n",
    "  no longer need to be checked for alignments and update the ignore vector accordingly. An\n",
    "  index can be ignored if the index and the l - 1 indices following it are already covered,\n",
    "  provided that none of those indices are concatenation spots.\n",
    "  '''\n",
    "  total_seqlen = len(cov)\n",
    "  current_seqstart = 0\n",
    "  current_seqend = 0\n",
    "  seqlens_ctr = -1\n",
    "\n",
    "  check_start = region[0] - l + 1\n",
    "  if check_start < 0:\n",
    "    check_start = 0\n",
    "  check_end = region[1] + l\n",
    "  if check_end > total_seqlen:\n",
    "    check_end = total_seqlen\n",
    "\n",
    "  streak = 0 # how many indices in a row are already covered.\n",
    "  for i in range(check_start, check_end):\n",
    "    while i >= current_seqend:\n",
    "      current_seqstart = current_seqend\n",
    "      seqlens_ctr += 1\n",
    "      current_seqend += seqlens[seqlens_ctr]\n",
    "      streak = 0 # if we cross a concatenation spot, streak is back to zero\n",
    "    if cov[i]:\n",
    "      streak += 1\n",
    "    else:\n",
    "      streak = 0\n",
    "    if streak >= l:\n",
    "      ignore[i - l + 1] = True\n",
    "\n",
    "def calculate_seqlens(seqs: List[str]) -> List[int]:\n",
    "  return [len(seq) for seq in seqs]\n",
    "\n",
    "def calculate_coverage(subs, l) -> List[Tuple[int, int]]:\n",
    "  '''\n",
    "  Given a list of substring starting indices and substring length, return an actual list of starting and ending indices for the coverage of these substrings.\n",
    "  Example: suppose we have subs = [5, 10, 15, 45], l = 10. These cover from [5, 25] and [45, 55]\n",
    "  '''\n",
    "  if len(subs) == 0:\n",
    "    return []\n",
    "  subs = sorted(subs)\n",
    "  results = []\n",
    "  curr_start = subs[0]\n",
    "  curr_end = subs[0] + l\n",
    "  for sub in subs[1:]:\n",
    "    if sub <= curr_end: # if this index is covered by the last started interval, extend the interval\n",
    "      curr_end = sub + l\n",
    "    else: # otherwise, end last one and start new one\n",
    "      results.append((curr_start, curr_end))\n",
    "      curr_start = sub\n",
    "      curr_end = sub + l\n",
    "  results.append((curr_start, curr_end)) # end last interval\n",
    "  return results\n",
    "\n",
    "def naive_alignment(\n",
    "  bait: str,\n",
    "  s_storage: np.ndarray,\n",
    "  d: int,\n",
    "  ignore: List[bool]\n",
    ") -> List[int]:\n",
    "  bait = np.array(list(bait))\n",
    "  l = len(bait)\n",
    "  result = []\n",
    "  for i in range(len(s_storage)):\n",
    "    if ignore[i]:\n",
    "      continue\n",
    "    distance = l - (s_storage[i, :] == bait).sum()\n",
    "    if distance <= d:\n",
    "      result.append(i)\n",
    "  return result\n",
    "\n",
    "def verify_baits(\n",
    "  baits: List[str],\n",
    "  s: Union[str, List[str]],\n",
    "  d: int,\n",
    "  log: str = None,\n",
    "):\n",
    "  if log is not None:\n",
    "    verbose = True\n",
    "    f = open(log, 'w')\n",
    "    f.write('Verifying baits with provided arguments:\\n')\n",
    "    f.write('d (mismatch allowance) = {}\\n'.format(d))\n",
    "    f.write('--------\\n')\n",
    "  else:\n",
    "    verbose = False\n",
    "\n",
    "  if isinstance(s, list):\n",
    "    seqlens = calculate_seqlens(s)\n",
    "    s = ''.join(s)\n",
    "  else:\n",
    "    seqlens = [len(s)]\n",
    "\n",
    "  length = len(s)\n",
    "  cov = [False] * length\n",
    "  l = len(baits[0])\n",
    "  ignore = initialize_ignore_vector(seqlens, l)\n",
    "  if verbose:\n",
    "    f.write('Initialized integer array and ignore vector with length {}\\n'.format(length))\n",
    "\n",
    "  ids = ['bait#{}'.format(str(i)) for i in range(len(baits))]\n",
    "\n",
    "  s_storage = np.empty((length - l + 1, l), dtype = str)\n",
    "  s = np.array(list(s))\n",
    "  for i in range(length - l + 1):\n",
    "    s_storage[i, :] = s[i: i + l]\n",
    "  for id, bait in zip(ids, baits):\n",
    "    if verbose:\n",
    "      f.write('Aligning bait {}.\\n'.format(id))\n",
    "    matches = naive_alignment(bait, s_storage, d, ignore)\n",
    "    coverages = calculate_coverage(matches, l)\n",
    "    if verbose:\n",
    "      f.write('Bait covers between:\\n {}\\n'.format(coverages))\n",
    "    for c in coverages:\n",
    "      for j in range(c[0], c[1]):\n",
    "        cov[j] = True\n",
    "  if verbose:\n",
    "    f.write('--------\\n')\n",
    "    f.write('Remaining uncovered indices: {}.\\n'.format(cov.count(False)))\n",
    "    f.write(str([i for i in range(len(cov)) if not cov[i]]))\n",
    "    f.close()\n",
    "  return [i for i in range(len(cov)) if not cov[i]]\n",
    "import random\n",
    "def similarity_index(\n",
    "  s: Union[str, List[str]],\n",
    "  d: int,\n",
    "  trials: int,\n",
    "):\n",
    "  if isinstance(s, list):\n",
    "    seqlens = calculate_seqlens(s)\n",
    "    s = ''.join(s)\n",
    "  else:\n",
    "    seqlens = [len(s)]\n",
    "\n",
    "  length = len(s)\n",
    "  cov = [False] * length\n",
    "  l=120\n",
    "  ignore = initialize_ignore_vector(seqlens, l)\n",
    "\n",
    "  s_storage = np.empty((length - l + 1, l), dtype = str)\n",
    "  s = np.array(list(s))\n",
    "  for i in range(length - l + 1):\n",
    "    s_storage[i, :] = s[i: i + l]\n",
    "  trial_counter=trials\n",
    "  similarities=0\n",
    "  while trial_counter > 0:\n",
    "    trial_counter-=1;\n",
    "    random_number1=random.randint(0,len(ignore)-1)#inclusive for both ends\n",
    "    random_number2=random.randint(0,len(ignore)-1)\n",
    "    if random_number1==random_number2:\n",
    "      trial_counter+=1\n",
    "      continue\n",
    "    if ignore[random_number1]==True or ignore[random_number2]==True:\n",
    "      trial_counter+=1\n",
    "      continue\n",
    "    distance = l - (s_storage[random_number1, :] == s_storage[random_number2, :]).sum()\n",
    "    if distance <= d:\n",
    "      similarities+=1\n",
    "  return similarities/trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a969533-c05d-4743-a043-22386a4b6f72",
   "metadata": {
    "id": "2a969533-c05d-4743-a043-22386a4b6f72"
   },
   "outputs": [],
   "source": [
    "def diagnosis(maxRadius,trials,inputFile):\n",
    "    start_time=time.time()\n",
    "    with open(inputFile, 'r') as file:\n",
    "        original = [line.strip() for line in file.readlines()]\n",
    "    print(\"Similarity Index:\",similarity_index(original,maxRadius,trials))\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f\"Diagnosis time for {inputFile}: {duration} seconds\")\n",
    "    return\n",
    "\n",
    "def run(windowLength,maxRadius,lenientRadius,lenientRadius2,overlapCount,bypassHyperparameter,searchBreadths,inputFile,outputFile,verification):\n",
    "    #NOTE: FILE PATHS MUST NOT HAVE SPACES\n",
    "    inp=str(windowLength)+\"\\n\"+str(maxRadius)+\"\\n\"+str(lenientRadius)+\"\\n\"+str(lenientRadius2)+\"\\n\"+str(overlapCount)+\"\\n\"+str(bypassHyperparameter)+\"\\n\"\n",
    "    inp+=str(len(searchBreadths))+\"\\n\"\n",
    "    for sb in searchBreadths:\n",
    "        inp+=str(sb)+\"\\n\"\n",
    "    inp+=inputFile+\"\\n\"\n",
    "    inp+=outputFile\n",
    "    cleaned_string = inp.replace(\"\\n\", \" \")\n",
    "    print(cleaned_string)\n",
    "\n",
    "    start_time = time.time()\n",
    "    os.chmod(inputFile, 0o777)\n",
    "    process=subprocess.Popen(\"./MultithreadedGenerativeSearchV4WithInput.exe\",\n",
    "                         stdin=subprocess.PIPE,\n",
    "                         stdout=subprocess.PIPE,\n",
    "                         stderr=subprocess.PIPE,\n",
    "                         text=True)\n",
    "    stdout, stderr = process.communicate(input=inp)\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f\"Execution time for {inputFile}: {duration} seconds\")\n",
    "    rt=duration\n",
    "    print(\"Errors (should be blank):\", stderr)\n",
    "    #print(\"inputFile: \",inputFile)\n",
    "    with open(inputFile, 'r') as file:\n",
    "        original = [line.strip() for line in file.readlines()]\n",
    "    with open(outputFile, 'r') as file:\n",
    "        baits = [line.strip() for line in file.readlines()]\n",
    "    #baits=baits[0:-1] to test true negative aka a bait list that doesnt cover\n",
    "    print(\"Number of baits:\", len(baits))\n",
    "    if verification:\n",
    "        print(\"Verifying\")\n",
    "        start_time = time.time()\n",
    "        missing_spots=verify_baits(baits,original,40)\n",
    "        if len(missing_spots)==0:\n",
    "            print(\"\\033[92m\", len(missing_spots), \"spots uncovered\\033[0m\")          \n",
    "        else:\n",
    "            print(\"\\033[91m\", len(missing_spots), \"spots uncovered\\033[0m\") \n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        print(f\"Verification time for {inputFile}: {duration} seconds\")\n",
    "    return [len(baits),rt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0fa2925-f6a8-4a53-9931-08916eef1c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modifiedFasta(length,inputFile,outputFile):\n",
    "    #NOTE: FILE PATHS MUST NOT HAVE SPACES\n",
    "    inp=str(length)+\"\\n\"+str(inputFile)+\"\\n\"+str(outputFile)\n",
    "    cleaned_string = inp.replace(\"\\n\", \" \")\n",
    "    print(cleaned_string)\n",
    "    start_time = time.time()\n",
    "    os.chmod(inputFile, 0o777)\n",
    "    process=subprocess.Popen(\"./dataCleaning.exe\",\n",
    "                         stdin=subprocess.PIPE,\n",
    "                         stdout=subprocess.PIPE,\n",
    "                         stderr=subprocess.PIPE,\n",
    "                         text=True)\n",
    "    stdout, stderr = process.communicate(input=inp)\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f\"Execution time for converting fasta: {duration} seconds\")\n",
    "    with open(outputFile, 'r') as file:\n",
    "        # Read the entire content of the file\n",
    "        content = file.read()\n",
    "    \n",
    "    # Get the length of the content\n",
    "    file_length = len(content)\n",
    "    \n",
    "    # Print the length\n",
    "    print(\"Number of nucelotides:\", file_length)\n",
    "    print(\"Errors (ignore ERROR! some values not ACGT):\", stderr)\n",
    "    os.chmod(outputFile, 0o777)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9124d8be-3c1d-4db0-a974-720291192a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numberProbes(inputFile,outputFile):\n",
    "    with open(inputFile, 'r') as input_file:\n",
    "        # Open the output file in write mode\n",
    "        with open(outputFile, 'w') as output_file:\n",
    "            i=1\n",
    "            # Read each line from the input file\n",
    "            for line in input_file:\n",
    "                # Write each line twice to the output file\n",
    "                output_file.write(\"probe-\"+str(i).zfill(8)+\"\\n\")\n",
    "                i+=1\n",
    "                output_file.write(line)  \n",
    "    print(f\"Finished writing to {outputFile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d49d817f-d4fe-4e6a-a9a1-a5b41e380b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterFasta(length,windowLength,maxRadius,lenientRadius,lenientRadius2,overlapCount,bypassHyperparameter,searchBreadths,inputFile,outputFile,verification):\n",
    "    modifiedFasta(length,inputFile,\"temporaryModifiedFastaFile.txt\")\n",
    "    run(windowLength,maxRadius,lenientRadius,lenientRadius2,overlapCount,bypassHyperparameter,searchBreadths,\"temporaryModifiedFastaFile.txt\",outputFile,verification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3da0e93-095b-4cc1-bb20-fef63b63c3dd",
   "metadata": {},
   "source": [
    "Run the 8 cells above this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d02c6ea4-ce3a-4835-b348-78b34fb694ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250000 NAFastaPartitions/final_NA_target_space_All.fasta temporaryModifiedFastaFile.txt\n",
      "Execution time for converting fasta: 0.018815994262695312 seconds\n",
      "Number of nucelotides: 251359\n",
      "Errors (ignore ERROR! some values not ACGT): \n"
     ]
    }
   ],
   "source": [
    "#calls .\\dataCleaning.exe, inputs are exactly the same as in the READ ME\n",
    "modifiedFasta(250000, \"NAFastaPartitions/final_NA_target_space_All.fasta\", \"temporaryModifiedFastaFile.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76d53f67-75a0-4c5a-ac92-9271227abb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 6 12 12 5 6 4 10 10 10 2 temporaryModifiedFastaFile.txt output.txt\n",
      "Execution time for temporaryModifiedFastaFile.txt: 6.980608224868774 seconds\n",
      "Errors (should be blank): \n",
      "Number of baits: 442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[442, 6.980608224868774]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calls .\\MultithreadedGenerativeSearchV4WithInput.exe, inputs are exactly the same as in the READ ME\n",
    "#with the added adition of a boolean of if you want to double check the generated baits completely cover the sequence\n",
    "run(60, 6,12, 12, 5, 6, [10,10,10,2], \"temporaryModifiedFastaFile.txt\", \"output.txt\", False)\n",
    "#killing one of these calls may require not only to press the stop button on jupyter notebook\n",
    "#but also going into task manager and manually ending the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e361408-7574-4834-9d46-4d3e785526a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 6 12 12 5 6 4 10 10 10 2 temporaryModifiedFastaFile.txt output.txt\n",
      "Execution time for temporaryModifiedFastaFile.txt: 6.9085681438446045 seconds\n",
      "Errors (should be blank): \n",
      "Number of baits: 442\n",
      "Verifying\n",
      "\u001b[92m 0 spots uncovered\u001b[0m\n",
      "Verification time for temporaryModifiedFastaFile.txt: 394.15737533569336 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[442, 6.9085681438446045]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example with calling the double check function, may take a long time (hours)\n",
    "run(60, 6,12, 12, 5, 6, [10,10,10,2], \"temporaryModifiedFastaFile.txt\", \"output.txt\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2558d45-8c16-4209-ba68-b75218bf34df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250000 NAFastaPartitions/final_NA_target_space_All.fasta temporaryModifiedFastaFile.txt\n",
      "Execution time for converting fasta: 0.019032716751098633 seconds\n",
      "Number of nucelotides: 251359\n",
      "Errors (ignore ERROR! some values not ACGT): \n",
      "60 6 12 12 5 6 4 10 10 10 2 temporaryModifiedFastaFile.txt output.txt\n",
      "Execution time for temporaryModifiedFastaFile.txt: 7.214697360992432 seconds\n",
      "Errors (should be blank): \n",
      "Number of baits: 443\n"
     ]
    }
   ],
   "source": [
    "#combines the modifiedFasta and run functions\n",
    "clusterFasta(250000, 60, 6, 12, 12, 5, 6, [10,10,10,2],\"NAFastaPartitions/final_NA_target_space_All.fasta\", \"output.txt\",False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e82737e-6294-493e-bcdb-a01c0b1ed40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing to numberedOutput.txt\n"
     ]
    }
   ],
   "source": [
    "#calls .\\numberProbes\n",
    "numberProbes(\"output.txt\",\"numberedOutput.txt\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
